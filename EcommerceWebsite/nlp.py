# -*- coding: utf-8 -*-
"""NLP Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TgAbzg6yROlF2_RDCcpPgIukarwVRAt-
"""

import spacy

from spacy.matcher import Matcher
from spacy.tokens import Span
from spacy import displacy

nlp = spacy.load('en_core_web_sm')

text="This is first sentence. and this is another one. here 3rd one is"

doc=nlp(text)

doc

for token in doc:
  print(token)

sent=nlp.create_pipe('sentencizer')

nlp.add_pipe(sent,before='parser')

doc=nlp(text)

for sent in doc.sents:
  print(sent)

from spacy.lang.en.stop_words import STOP_WORDS

stopwords=list(STOP_WORDS)

print(stopwords)

for word in doc:
  print(word)

for token in doc:
  if token.is_stop==False:
    print(token)

## Lemmatization

doc=nlp('run running runs runner')

for token in doc:
  print(doc)

for len in doc:
  print(len.text,len.lemma_)

## POS

doc=nlp('All is well at your end!')

for token in doc:
  print(token.text,token.pos_
        )

displacy.render(doc,style='dep')

## Entity Detection

doc=nlp('New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate aganist dangerous diseases.At least 285 people have contracted measles in the city since September,mostly in Brooklyn Williamsbury neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday. The mandate orders all unvaccinated people in the area, including a concentration of Orthod ox Jews, to recieve inoculations, including for children as young as 6 months old. Anyone who resists could be fined upto $1000.')

doc

displacy.render(doc, style = 'ent')

## Text Classification

import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

data = pd.read_csv('final_dataset_analysis.csv')

data.head()

data.shape

data['sentiment'].value_counts()

data.isnull().sum()

## Tokenization

import string

puct=string.punctuation

puct

def text_data_cleaning(sentence):
    doc = nlp(sentence)
    
    tokens = []
    for token in doc:
        if token.lemma_ != "-PRON-":
            temp = token.lemma_.lower().strip()
        else:
            temp = token.lower_
        tokens.append(temp)
    
    cleaned_tokens = []
    for token in tokens:
        if token not in stopwords and token not in puct:
            cleaned_tokens.append(token)
    return cleaned_tokens

corpus=[]
for i in range(0,28332):
  temp=text_data_cleaning(data['text'][i])
  corpus.append(temp)

corpus[1]

corpus[0]

ans=[]
for sent in corpus:
  temp=""
  for word in sent:
    temp=temp+' '+word
  ans.append(temp)

ans[1]

## Vectorication Feature Engineering

from sklearn.svm import LinearSVC



tfidf = TfidfVectorizer()
classifier = LinearSVC()

X = data['text']
y = data['sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train.shape, X_test.shape

clf = Pipeline([('tfidf', tfidf.fit(ans)), ('clf', classifier)])







clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))

confusion_matrix(y_test, y_pred)

clf.predict(['Wow, this is amzing lesson'])

clf.predict(['It is bad, do not take it you will waste your money'])

clf.predict(['is always stops working bad product worst'])

clf.predict(['Take it'])

clf.predict(['worst '])

clf.predict(['this was better than the previous , but not the best ofcourse,needs improvement as it is not working properly'])

import pickle
pickl = {
    'classifier': clf,
}
pickle.dump( pickl, open( 'models' + ".p", "wb" ) )

accuracy_score(y_test, y_pred)











